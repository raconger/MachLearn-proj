---
title: "Machine Learning Project"
author: "raconger"
date: "November 22, 2015"
output: html_document
#    pandoc_args: [
#      "+RTS", "-K64m",
#      "-RTS"
#    ]
---

This is an R Markdown document to describe the work done on the Machine Learning course project.
## Data Cleaning and Exploration

One may gather the data by using the following block of code: 

```{r}
library(caret)
library(AppliedPredictiveModeling)
library(rattle)
library(rpart.plot)
library(corrplot)

trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

traindest <- "train.csv"
download.file(trainURL, traindest, method = "curl")    
traindata <- read.csv(traindest, sep = ",")

testdest <- "test.csv"
download.file(testURL, testdest, method = "curl")    
testdata <- read.csv(testdest, sep = ",")
```

## Model building and Cross Validation

Given the discrete values in the classe variable, a random forest will be used to build the model. To be able to evaluate model efficacy, a training and a test set are to be developed to allow for an effective cross validation. I chose to use 70% of data for training, and 30% for validation.

```{r}
clean <- na.omit(traindata)
inTrain <- createDataPartition(traindata$classe, p = .7, list = FALSE)
trainMod <- traindata[inTrain,]
validMod <- traindata[-inTrain,]
```

It can be observed that many of the columns or sensor variables are not returning valid data. As such, a model should be developed only by valid data that may effectively influence the model and not increase the complexity unnecessarily.

```{r}
trainRF <- train(classe ~ accel_belt_x + accel_belt_y + magnet_arm_x + 
                  accel_belt_z + total_accel_arm + roll_dumbbell + pitch_dumbbell 
                  + yaw_dumbbell + roll_belt + pitch_belt + yaw_belt 
                  + total_accel_dumbbell +total_accel_forearm +total_accel_belt 
                 ,method="rpart",data=trainMod)

```

Upon creation of the model, it can then be viewed:

```{r}
fancyRpartPlot(trainRF$finalModel, sub = "Graphical Illustration of Model")
# plot(trainRF$finalModel, uniform=T) 
# text(trainRF$finalModel, cex=0.8)
```

## Expected Error

Now that the model has been created, it can be checked for accuracy. In this case, let's use a confusion matrix and then assess overall model accuracy.


```{r}
validationTest <- predict(trainRF, validMod)
confusionMatrix(validationTest, validMod$classe)

validationPred <- predict(trainRF, testdata)
```

It can be observed that the model itself does not have a high degree of accuracy. This is likely due to two factors. First, the data should be cleaned more effectively and second, perhaps more input parameters should be used, or a different model should be used.
